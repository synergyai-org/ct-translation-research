{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ruamel.yaml import YAML\n",
    "import h5py\n",
    "yaml = YAML()\n",
    "with open('/home/synergyai/jth/rcc-classification-research/experiments/config/session_240621.yaml') as f:\n",
    "    config = yaml.load(f)\n",
    "\n",
    "split_dict = {}\n",
    "with open(config['path_split'],'r') as f_split:\n",
    "    split = f_split.read().splitlines()\n",
    "    for i, split_name in enumerate(['train', 'valid', 'test']):\n",
    "        string = split[i]\n",
    "        string = string.replace('[','').replace(']','').replace(\"'\", \"\")\n",
    "        id_list = string.split(', ')\n",
    "        split_dict[split_name] = id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(config['path_data'], 'r', swmr=True) as f:\n",
    "    start_indexes = []\n",
    "    accumulated_count = 0\n",
    "    list_id = []\n",
    "    for id in split_dict['train']:\n",
    "        try: \n",
    "            shape_pre = f[id]['image']['precontrast_r']['Axial'].shape\n",
    "            shape_post = f[id]['image']['post_50sec']['Axial'].shape\n",
    "            shape_late = f[id]['image']['post_5min_r']['Axial'].shape\n",
    "            shape_mask = f[id]['segmentation']['post_50sec']['Axial'].shape\n",
    "        except KeyError:\n",
    "            #print(f'Error in {id}')\n",
    "            continue\n",
    "\n",
    "        assert(shape_pre == shape_post == shape_late == shape_mask)\n",
    "        start_indexes.append(accumulated_count)\n",
    "        accumulated_count += shape_pre[0]\n",
    "        list_id.append(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "class rcc_3d_dataset(Dataset):\n",
    "    def __init__(self, list_id, config):\n",
    "        self.config = config\n",
    "\n",
    "        with h5py.File(config['path_data'], 'r', swmr=True) as f:\n",
    "            start_indexes = []\n",
    "            accumulated_count = 0\n",
    "            new_list_id = []\n",
    "            for id in list_id:\n",
    "                try: \n",
    "                    shape_pre = f[id]['image']['precontrast_r']['Axial'].shape\n",
    "                    shape_post = f[id]['image']['post_50sec']['Axial'].shape\n",
    "                    shape_late = f[id]['image']['post_5min_r']['Axial'].shape\n",
    "                    shape_mask = f[id]['segmentation']['post_50sec']['Axial'].shape\n",
    "                except KeyError:\n",
    "                    continue\n",
    "\n",
    "                assert(shape_pre == shape_post == shape_late == shape_mask)\n",
    "                start_indexes.append(accumulated_count)\n",
    "                accumulated_count += shape_pre[0]\n",
    "                new_list_id.append(id)\n",
    "\n",
    "        self.list_id = new_list_id\n",
    "        self.start_indexes = start_indexes ##\n",
    "        self.tot_count = accumulated_count ##\n",
    "        self.path_data = config['path_data']\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tot_count\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        id_index = np.digitize(idx, self.start_indexes)-1\n",
    "        slice_index = idx - self.start_indexes[id_index]\n",
    "        image_x = h5py.File(self.path_data, 'r', swmr=True)[self.list_id[id_index]]['image']['precontrast_r']['Axial'][slice_index]\n",
    "        image_y = h5py.File(self.path_data, 'r', swmr=True)[self.list_id[id_index]]['image']['post_50sec']['Axial'][slice_index]\n",
    "\n",
    "        image_x = torch.from_numpy((image_x/2000).astype(np.float32))\n",
    "        image_y = torch.from_numpy((image_y/2000).astype(np.float32))\n",
    "\n",
    "        return (image_x, image_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48925"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class rcc_dataset(Dataset):\n",
    "    def __init__(self, image_path, label_path, verbose = True, standardize = True):\n",
    "        self.label_memmap = np.memmap(label_path, dtype=np.int16, mode='r')\n",
    "        length = self.label_memmap.shape[0]\n",
    "        self.image_memmap = np.memmap(image_path, dtype=np.float32, mode='r', shape = (length, 3, 224, 224))\n",
    "        self.malig_count = np.count_nonzero(self.label_memmap)\n",
    "        self.benign_count = length - self.malig_count\n",
    "        if verbose:\n",
    "            print(f'Malignant slices: {self.malig_count}, Benign slices: {self.benign_count}')\n",
    "\n",
    "        # self.mean, std carries params for z-standardization. may not be equal to the actual mean, std of the dataset\n",
    "        self.mean = np.mean(self.image_memmap)\n",
    "        self.std = np.std(self.image_memmap)\n",
    "        self.standardize = standardize\n",
    "\n",
    "        self.target_mean = 0.449\n",
    "        self.target_std = 0.226\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label_memmap)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.image_memmap[idx].copy()\n",
    "        if self.standardize:\n",
    "            image = (image - self.mean) / self.std * self.target_std + self.target_mean\n",
    "\n",
    "        label = self.label_memmap[idx].copy()\n",
    "        label = np.array([label])\n",
    "        return torch.from_numpy(image), torch.from_numpy(label)\n",
    "    \n",
    "    def get_z_params(self):\n",
    "        print(f'Actual mean: {np.mean(self.image_memmap)}, Actual std: {np.std(self.image_memmap)}')\n",
    "        return np.mean(self.image_memmap), np.std(self.image_memmap)\n",
    "\n",
    "    def set_z_params(self, mean, std):\n",
    "        print(f'Setting mean {self.mean} to {mean}, std {self.std} to {std} for z-standardization')\n",
    "        self.standardize = True\n",
    "        self.mean = mean\n",
    "        self.std = std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.memmap(config['path_data'], dtype='float32', mode='r', shape=(len(split_dict['train']), 3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateDatasetSynthesis(phase, input_path, contrast1 = 'T1', contrast2 = 'T2'):\n",
    "\n",
    "    target_file = input_path + \"/data_{}_{}.mat\".format(phase, contrast1)\n",
    "    data_fs_s1=LoadDataSet(target_file)\n",
    "    \n",
    "    target_file = input_path + \"/data_{}_{}.mat\".format(phase, contrast2)\n",
    "    data_fs_s2=LoadDataSet(target_file)\n",
    "\n",
    "    dataset=torch.utils.data.TensorDataset(torch.from_numpy(data_fs_s1),torch.from_numpy(data_fs_s2))  \n",
    "    return dataset \n",
    "\n",
    "\n",
    "\n",
    "#Dataset loading from load_dir and converintg to 256x256 \n",
    "def LoadDataSet(load_dir, variable = 'data_fs', padding = True, Norm = True):\n",
    "    f = h5py.File(load_dir,'r') \n",
    "    if np.array(f[variable]).ndim==3:\n",
    "        data=np.expand_dims(np.transpose(np.array(f[variable]),(0,2,1)),axis=1)\n",
    "    else:\n",
    "        data=np.transpose(np.array(f[variable]),(1,0,3,2))\n",
    "    data=data.astype(np.float32) \n",
    "    if padding:\n",
    "        pad_x=int((256-data.shape[2])/2)\n",
    "        pad_y=int((256-data.shape[3])/2)\n",
    "        print('padding in x-y with:'+str(pad_x)+'-'+str(pad_y))\n",
    "        data=np.pad(data,((0,0),(0,0),(pad_x,pad_x),(pad_y,pad_y)))   \n",
    "    if Norm:    \n",
    "        data=(data-0.5)/0.5      \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 dataset \"data_fs\": shape (25, 152, 256), type \"<f4\">\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "from ruamel.yaml import YAML\n",
    "\n",
    "with h5py.File('T2.mat','r') as f:\n",
    "    print(f['data_fs'])\n",
    "\n",
    "yaml = YAML()\n",
    "with open('config.yaml','r') as f:\n",
    "    config = yaml.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dict = {}\n",
    "with open(config['path_split'],'r') as f_split:\n",
    "    split = f_split.read().splitlines()\n",
    "for i, split_name in enumerate(['train', 'valid', 'test']):\n",
    "    string = split[i]\n",
    "    string = string.replace('[','').replace(']','').replace(\"'\", \"\")\n",
    "    id_list = string.split(', ')\n",
    "    split_dict[split_name] = id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('/merged_storage/jth/data_0612_backup.h5', 'r') as f:\n",
    "    count_unmatched = 0\n",
    "    count_matched = 0 \n",
    "\n",
    "    new_split_dict = {}\n",
    "    for split_name, list_id in split_dict.items():\n",
    "        new_list_id = list_id.copy() \n",
    "        for id in list_id:\n",
    "            g_sample = f[id]\n",
    "            try:\n",
    "                # segmentation이 있는 것만 count.\n",
    "                segment = g_sample['segmentation/post_50sec/Axial']\n",
    "                pre_image = g_sample['image/precontrast/Axial']\n",
    "                post_image = g_sample['image/post_50sec/Axial']\n",
    "                if(pre_image.shape != post_image.shape):\n",
    "                    #print(id, pre_image.shape, post_image.shape)\n",
    "                    count_unmatched +=1\n",
    "                    new_list_id.remove(id)\n",
    "                else:\n",
    "                    count_matched +=1\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        new_split_dict[split_name] = new_list_id\n",
    "\n",
    "print(f'count_unmatched: {count_unmatched}, count_matched: {count_matched}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "401"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_split_dict['train'].__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = split_dict['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('/merged_storage/jth/data_0612_backup.h5', 'r') as f:\n",
    "    g_sample = f[id]\n",
    "    segment = g_sample['segmentation/post_50sec/Axial'][:]\n",
    "    pre_image = g_sample['image/precontrast/Axial'][:]\n",
    "    post_image = g_sample['image/post_50sec/Axial'][:]\n",
    "    late_image = g_sample['image/post_5min/Axial'][:]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  21,   38,   30, ...,   16,   22,   38],\n",
       "        [  15,   25,   27, ...,   26,   34,   50],\n",
       "        [  26,   22,   25, ...,   31,   34,   33],\n",
       "        ...,\n",
       "        [1226, 1102,  921, ..., 1164, 1089,  813],\n",
       "        [1205, 1205, 1122, ..., 1073,  820,  470],\n",
       "        [1247, 1262, 1226, ...,  765,  455,  259]],\n",
       "\n",
       "       [[  20,   38,   33, ...,    0,   24,   41],\n",
       "        [  13,   28,   30, ...,   25,   30,   27],\n",
       "        [  20,   26,   27, ...,   31,   30,   22],\n",
       "        ...,\n",
       "        [1251, 1119,  933, ..., 1171, 1088,  830],\n",
       "        [1230, 1220, 1130, ..., 1072,  823,  478],\n",
       "        [1246, 1264, 1264, ...,  773,  462,  261]],\n",
       "\n",
       "       [[  17,   33,   28, ...,    0,   17,   32],\n",
       "        [  27,   30,   27, ...,   24,   30,   45],\n",
       "        [  18,   29,   28, ...,   29,   29,   12],\n",
       "        ...,\n",
       "        [1282, 1110,  914, ..., 1175, 1092,  855],\n",
       "        [1252, 1212, 1113, ..., 1081,  825,  451],\n",
       "        [1216, 1248, 1262, ...,  806,  441,  267]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[  31,   19,    1, ...,   34,   24,   20],\n",
       "        [  18,   16,   20, ...,   23,   26,   33],\n",
       "        [   8,   14,   18, ...,   22,   24,   33],\n",
       "        ...,\n",
       "        [ 826,  563,  357, ..., 1042, 1192, 1282],\n",
       "        [1122,  822,  581, ..., 1188, 1113,  865],\n",
       "        [1219, 1132,  852, ..., 1229,  829,  495]],\n",
       "\n",
       "       [[  17,   11,    3, ...,   25,   28,   23],\n",
       "        [  24,   11,   14, ...,   22,   29,   39],\n",
       "        [  15,   15,   15, ...,   25,   29,   34],\n",
       "        ...,\n",
       "        [ 819,  557,  348, ..., 1044, 1198, 1310],\n",
       "        [1138,  820,  573, ..., 1194, 1111,  862],\n",
       "        [1221, 1118,  850, ..., 1254,  820,  463]],\n",
       "\n",
       "       [[  28,   16,   33, ...,   18,   33,   27],\n",
       "        [  11,   17,   20, ...,   27,   34,   37],\n",
       "        [  30,   19,   20, ...,   28,   29,   28],\n",
       "        ...,\n",
       "        [ 783,  539,  331, ..., 1041, 1186, 1279],\n",
       "        [1108,  801,  549, ..., 1190, 1104,  854],\n",
       "        [1258, 1132,  830, ..., 1201,  814,  456]]], dtype=uint16)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
